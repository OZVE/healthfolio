import json
import logging
import os
import asyncio
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from redis import Redis

# Configurar logging
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv(Path(__file__).parent.parent / ".env")

# Memoria en RAM como fallback
memory_fallback = {}

# Sistema de batching de mensajes
message_batches = {}  # chat_id -> {messages: [], last_update: timestamp, task: asyncio.Task}
BATCH_TIMEOUT = 3.0  # segundos de espera antes de procesar
MAX_BATCH_SIZE = 10  # mÃ¡ximo nÃºmero de mensajes por batch

def get_redis_client():
    """Obtiene cliente Redis o None si no estÃ¡ configurado."""
    redis_url = os.getenv("REDIS_URL")
    
    if not redis_url:
        logger.info("ðŸ”„ Redis no configurado, usando memoria en RAM")
        return None
        
    try:
        client = Redis.from_url(redis_url)
        client.ping()
        logger.info("âœ… Redis conectado exitosamente")
        return client
        
    except Exception as e:
        logger.error(f"âŒ Error conectando a Redis: {e}")
        logger.info("ðŸ”„ Usando memoria en RAM como fallback")
        return None

# Inicializar Redis
redis_client = get_redis_client()
logger.info(f"ðŸ’¾ Sistema de memoria: {'Redis' if redis_client else 'RAM (fallback)'}")


def extract_text_from_event(event: Dict[str, Any]) -> str | None:
    """Intenta obtener el texto del mensaje entrante."""
    data = event.get("data", {})
    msg_obj = data.get("message", {})
    # Formatos posibles: conversation (texto plano) o extendedTextMessage
    if "conversation" in msg_obj:
        return msg_obj["conversation"]
    if "extendedTextMessage" in msg_obj:
        return msg_obj["extendedTextMessage"].get("text")
    return None


def extract_text_from_twilio_event(form_data: Dict[str, Any]) -> str | None:
    """Extrae el texto del mensaje de un webhook de Twilio."""
    return form_data.get("Body")


def get_chat_id(event: Dict[str, Any]) -> str:
    """Extrae el nÃºmero E164 de remoteJid."""
    remote = event.get("data", {}).get("key", {}).get("remoteJid", "")
    chat_id = remote.split("@")[0]
    logger.info(f"ðŸ“± Chat ID extraÃ­do de Evolution: '{remote}' -> '{chat_id}'")
    return chat_id


def get_twilio_chat_id(form_data: Dict[str, Any]) -> str:
    """Extrae el nÃºmero de telÃ©fono del webhook de Twilio."""
    from_number = form_data.get("From", "")
    # Remover el prefijo "whatsapp:" si estÃ¡ presente
    chat_id = from_number.replace("whatsapp:", "").replace("+", "")
    logger.info(f"ðŸ“± Chat ID extraÃ­do de Twilio: '{from_number}' -> '{chat_id}'")
    return chat_id


def get_memory(chat_id: str) -> List[Dict]:
    key = f"mem:{chat_id}"
    logger.info(f"ðŸ” Obteniendo memoria para chat_id: '{chat_id}' (key: '{key}')")
    
    if redis_client:
        try:
            data = redis_client.get(key)
            if data:
                history = json.loads(data)
                logger.info(f"âœ… Memoria recuperada de Redis: {len(history)} mensajes")
                return history
            else:
                logger.info(f"ðŸ“­ No hay memoria en Redis para key: '{key}'")
                return []
        except Exception as e:
            logger.error(f"âŒ Error obteniendo memoria de Redis: {e}")
            
    # Fallback a memoria en RAM
    history = memory_fallback.get(key, [])
    logger.info(f"ðŸ”„ Usando memoria en RAM: {len(history)} mensajes")
    return history


def set_memory(chat_id: str, messages: List[Dict]):
    key = f"mem:{chat_id}"
    logger.info(f"ðŸ’¾ Guardando memoria para chat_id: '{chat_id}' (key: '{key}') - {len(messages)} mensajes")
    
    if redis_client:
        try:
            redis_client.setex(key, 14 * 24 * 3600, json.dumps(messages))
            logger.info(f"âœ… Memoria guardada en Redis exitosamente")
            return
        except Exception as e:
            logger.error(f"âŒ Error guardando memoria en Redis: {e}")
            
    # Fallback a memoria en RAM
    memory_fallback[key] = messages
    logger.info(f"ðŸ”„ Memoria guardada en RAM (total de chats: {len(memory_fallback)})")
    # Limpiar memoria vieja (mantener solo 100 chats)
    if len(memory_fallback) > 100:
        oldest_key = next(iter(memory_fallback))
        del memory_fallback[oldest_key]
        logger.info(f"ðŸ§¹ Memoria vieja limpiada: {oldest_key}")


class MessageBatcher:
    """Sistema de batching de mensajes para agrupar mensajes consecutivos."""
    
    def __init__(self):
        self.batches = {}  # chat_id -> {messages: [], last_update: timestamp, task: Optional[asyncio.Task]}
        self.processing_callbacks = {}  # chat_id -> callback function
    
    def add_message(self, chat_id: str, message: str, callback) -> bool:
        """
        Agrega un mensaje al batch y programa su procesamiento.
        
        Args:
            chat_id: ID del chat
            message: Mensaje del usuario
            callback: FunciÃ³n a llamar cuando se procese el batch
            
        Returns:
            True si el mensaje se agregÃ³ al batch, False si se procesÃ³ inmediatamente
        """
        current_time = time.time()
        
        # Si no hay batch activo para este chat, crear uno nuevo
        if chat_id not in self.batches:
            self.batches[chat_id] = {
                "messages": [],
                "last_update": current_time,
                "task": None
            }
            self.processing_callbacks[chat_id] = callback
        
        batch = self.batches[chat_id]
        batch["messages"].append(message)
        batch["last_update"] = current_time
        
        logger.info(f"ðŸ“¦ Mensaje agregado al batch para {chat_id}: '{message[:50]}...' (total: {len(batch['messages'])})")
        
        # Si alcanzamos el tamaÃ±o mÃ¡ximo, procesar inmediatamente
        if len(batch["messages"]) >= MAX_BATCH_SIZE:
            logger.info(f"ðŸš€ Batch completo para {chat_id}, procesando inmediatamente")
            self._process_batch(chat_id)
            return False
        
        # Cancelar tarea anterior si existe
        if batch["task"] and not batch["task"].done():
            batch["task"].cancel()
            logger.debug(f"ðŸ”„ Tarea anterior cancelada para {chat_id}")
        
        # Programar nueva tarea
        batch["task"] = asyncio.create_task(self._schedule_batch_processing(chat_id))
        logger.debug(f"â° Tarea programada para {chat_id} en {BATCH_TIMEOUT} segundos")
        
        return True
    
    async def _schedule_batch_processing(self, chat_id: str):
        """Espera el timeout y luego procesa el batch."""
        try:
            await asyncio.sleep(BATCH_TIMEOUT)
            
            # Verificar si el batch aÃºn existe y no ha sido procesado
            if chat_id in self.batches:
                batch = self.batches[chat_id]
                current_time = time.time()
                
                # Solo procesar si han pasado suficientes segundos desde el Ãºltimo mensaje
                if current_time - batch["last_update"] >= BATCH_TIMEOUT:
                    logger.info(f"â° Timeout alcanzado para {chat_id}, procesando batch")
                    self._process_batch(chat_id)
                else:
                    logger.debug(f"â° Timeout alcanzado pero batch actualizado recientemente para {chat_id}")
                    
        except asyncio.CancelledError:
            logger.debug(f"ðŸ”„ Tarea cancelada para {chat_id}")
        except Exception as e:
            logger.error(f"âŒ Error en schedule_batch_processing para {chat_id}: {e}")
    
    def _process_batch(self, chat_id: str):
        """Procesa el batch de mensajes."""
        if chat_id not in self.batches:
            logger.warning(f"âš ï¸ No se encontrÃ³ batch para {chat_id}")
            return
        
        batch = self.batches[chat_id]
        callback = self.processing_callbacks.get(chat_id)
        
        if not callback:
            logger.error(f"âŒ No se encontrÃ³ callback para {chat_id}")
            return
        
        # Combinar todos los mensajes en uno solo
        combined_message = self._combine_messages(batch["messages"])
        logger.info(f"ðŸ”„ Procesando batch para {chat_id}: {len(batch['messages'])} mensajes combinados")
        logger.debug(f"ðŸ“ Mensaje combinado: '{combined_message[:100]}...'")
        
        # Limpiar batch
        del self.batches[chat_id]
        if chat_id in self.processing_callbacks:
            del self.processing_callbacks[chat_id]
        
        # Procesar el mensaje combinado
        try:
            # Crear una tarea asÃ­ncrona para ejecutar el callback
            asyncio.create_task(self._execute_callback(callback, combined_message))
        except Exception as e:
            logger.error(f"âŒ Error procesando batch para {chat_id}: {e}")
    
    async def _execute_callback(self, callback, combined_message):
        """Ejecuta el callback de manera asÃ­ncrona."""
        try:
            await callback(combined_message)
        except Exception as e:
            logger.error(f"âŒ Error ejecutando callback: {e}")
    
    def _combine_messages(self, messages: List[str]) -> str:
        """
        Combina mÃºltiples mensajes en uno solo de manera inteligente.
        
        Estrategias:
        1. Si son saludos + solicitud, combinar naturalmente
        2. Si son frases incompletas, unirlas
        3. Si son mensajes completos, separar con espacios
        """
        if len(messages) == 1:
            return messages[0]
        
        combined = []
        current_phrase = []
        
        for i, message in enumerate(messages):
            message = message.strip()
            
            # Detectar saludos
            if message.lower() in ["hola", "buenos dÃ­as", "buenas", "buenas tardes", "buenas noches", "muchas gracias", "gracias"]:
                if current_phrase:
                    combined.append(" ".join(current_phrase))
                    current_phrase = []
                combined.append(message)
                continue
            
            # Detectar frases incompletas (terminan con preposiciones, artÃ­culos, etc.)
            incomplete_endings = ["de", "en", "con", "para", "por", "sin", "sobre", "entre", "hacia", "hasta", "desde", "durante", "mediante", "segÃºn", "un", "una", "el", "la", "los", "las", "este", "esta", "estos", "estas", "ese", "esa", "esos", "esas", "aquel", "aquella", "aquellos", "aquellas"]
            
            if any(message.lower().endswith(f" {ending}") for ending in incomplete_endings):
                current_phrase.append(message)
            else:
                current_phrase.append(message)
                if current_phrase:
                    combined.append(" ".join(current_phrase))
                    current_phrase = []
        
        # Agregar cualquier frase pendiente
        if current_phrase:
            combined.append(" ".join(current_phrase))
        
        result = " ".join(combined)
        logger.debug(f"ðŸ”— Mensajes combinados: {messages} -> '{result}'")
        return result
    
    def get_batch_status(self, chat_id: str) -> Optional[Dict]:
        """Obtiene el estado actual del batch para un chat."""
        if chat_id not in self.batches:
            return None
        
        batch = self.batches[chat_id]
        return {
            "message_count": len(batch["messages"]),
            "last_update": batch["last_update"],
            "time_since_last": time.time() - batch["last_update"],
            "messages": batch["messages"]
        }
    
    def force_process(self, chat_id: str):
        """Fuerza el procesamiento inmediato del batch."""
        if chat_id in self.batches:
            logger.info(f"âš¡ Forzando procesamiento de batch para {chat_id}")
            self._process_batch(chat_id)


# Instancia global del batcher
message_batcher = MessageBatcher()